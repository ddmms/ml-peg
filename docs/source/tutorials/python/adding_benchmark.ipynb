{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8a0986",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ddmms/ml-peg/blob/main/docs/source/tutorials/python/adding_benchmark.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c43a8f",
   "metadata": {},
   "source": [
    "## Adding a benchmark to ML-PEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147716a",
   "metadata": {},
   "source": [
    "This notebook guides you through the process of adding a new benchmark to ML-PEG with interactive capabilities. The benchmark added will have the simplest form of interactivity, which is table -> scatter -> structure, meaning clicking a table cell shows a scatter plot from which the errors were calculated, and clicking a point on the scatter plot shows the corresponding structure. Different types (e.g. density plots) or more advanced interactivity (table -> scatter -> phonon dispersion) can also be achieved but will not be covered in this first introduction.\n",
    "\n",
    "The workflow consists of 3 main steps + documentation:\n",
    "- **calc**: where the benchmarking script is defined, typically outputting results as .xyz files (e.g. energies for a structure are stored as properties in the .xyz file)\n",
    "- **analysis**: takes the outputs from calc, computes relevant metrics, builds tables and plots (saved as .json files).\n",
    "- **app**: builds the interactive app using the analysis output plots/tables and structure files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a492f39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d2f64",
   "metadata": {},
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557513cd",
   "metadata": {},
   "source": [
    "- Install pre-commit hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce721fe",
   "metadata": {},
   "source": [
    "### GitHub\n",
    "\n",
    "To contribute a new benchmark, first open an issue on GitHub by going to Issues -> New issue -> New benchmark, where you can briefly describe the benchmark you wish to add. After you have created the issue, click on the issue and assign yourself to it (top RHS of page under \"Assignees\").\n",
    "\n",
    "You can then create a new branch for your benchmark:\n",
    "\n",
    "```bash\n",
    "git checkout main\n",
    "git pull origin main\n",
    "git checkout -b my_new_benchmark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c305727",
   "metadata": {},
   "source": [
    "### Setting up your directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d841459",
   "metadata": {},
   "source": [
    "In the ML-PEG, we organise benchmarks into *categories*, which make up the tabs in the app, along with the summary page. For example, the Molecular Crystals category consists of molecular crystal benchmarks e.g. DMC-ICE13 and X23.\n",
    "\n",
    "First see if your benchmark fits into an existing category. If so, create a new benchmark folder in the relevant category folders in `ml_peg/calcs/<category>/`, `ml_peg/analysis/<category>/` and `ml_peg/app/<category>/`, and move onto the next steps.\n",
    "\n",
    "If not, create a new category folder in `ml_peg/calcs/`, `ml_peg/analysis/` and `ml_peg/app/`. You'll also need to create a simple app config file in `ml_peg/app/<category>/` e.g. `ml_peg/app/molecular_crystal/molecular_crystal.yml`:\n",
    "```yaml\n",
    "    title: Molecular Crystals\n",
    "    description: Formation energies of molecular crystals\n",
    "```\n",
    "and add the category directory name to `docs/source/user_guide/benchmarks/index.rst` so that it appears in the documentation sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2666663",
   "metadata": {},
   "source": [
    "### 0. Documentation\n",
    "Before adding a new benchmark, it is good practice to add a documentation page for it. You can follow the format of existing benchmark documentation pages in `docs/source/user_guide/benchmarks/`. This is the example for the X23 molecular crystal benchmark we will look at in this notebook `docs/source/user_guide/benchmarks/molecular_crystals.rst`: \n",
    "\n",
    "```rst\n",
    "    ==================\n",
    "    Molecular Crystals\n",
    "    ==================\n",
    "\n",
    "    X23\n",
    "    ===\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "\n",
    "    Performance in predicting lattice energies for 23 molecular crystals.\n",
    "\n",
    "\n",
    "    Metrics\n",
    "    -------\n",
    "\n",
    "    1. Lattice energy error\n",
    "\n",
    "    Accuracy of lattice energy predictions.\n",
    "\n",
    "    For each molecular crystal, lattice energy is calculated by taking the difference\n",
    "    between the energy of the solid molecular crystal divided by the number of molecules it\n",
    "    comprises, and the energy of the isolated molecule. This is compared to the reference\n",
    "    lattice energy.\n",
    "\n",
    "\n",
    "    Computational cost\n",
    "    ------------------\n",
    "\n",
    "    Low: tests are likely to take less than a minute to run on CPU.\n",
    "\n",
    "\n",
    "    Data availability\n",
    "    -----------------\n",
    "\n",
    "    Input structures:\n",
    "\n",
    "    * A. M. Reilly and A. Tkatchenko, Understanding the role of vibrations, exact exchange,\n",
    "    and many-body van der waals interactions in the cohesive properties of molecular\n",
    "    crystals, The Journal of chemical physics 139 (2013).\n",
    "\n",
    "    Reference data:\n",
    "\n",
    "    * Same as input data\n",
    "    * DMC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3e32c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e03ed5",
   "metadata": {},
   "source": [
    "### 1. Calc step: defining the benchmarking script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac7bd7",
   "metadata": {},
   "source": [
    "First create a directory for your benchmark in `ml_peg/calcs/<category>/your_benchmark/` and create a new Python script `calc_your_benchmark.py` in this directory. This is where you will define your benchmarking script. Please use underscores, not hyphens, in the benchmark name to avoid import errors when building the final app.\n",
    "\n",
    "Now prepare your imports, load models (from `models/models.yml`), define data and output paths, and any unit conversions you may need. This setup is fairly standard across all benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run calculations for X23 tests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from ase import units\n",
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "from ml_peg.calcs.utils.utils import download_s3_data\n",
    "from ml_peg.models.get_models import load_models\n",
    "from ml_peg.models.models import current_models\n",
    "\n",
    "MODELS = load_models(current_models)\n",
    "\n",
    "DATA_PATH = Path(__file__).parent / \"data\"\n",
    "OUT_PATH = Path(__file__).parent / \"outputs\"\n",
    "\n",
    "# Unit conversion\n",
    "EV_TO_KJ_PER_MOL = units.mol / units.kJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb19d5",
   "metadata": {},
   "source": [
    "Now you can define your benchmarking function. We use `pytest` to run all benchmarks in ML-PEG, so the function **must** begin with `test_` to be automatically discovered by `pytest`. A `pytest` decorator is also used to loop this test over all specified models.\n",
    "\n",
    "For this example, `add_d3_calculator` is used to add dispersion corrections to models where applicable. This is being expanded to include other dispersion corrections.\n",
    "\n",
    "Input data: For development, place your input data in your cache folder e.g. `\"~/.cache/ml-peg/X23/lattice_energy\"`. When the benchmark is finalised and ready to be merged with the main branch, please ask for your data to be uploaded to the S3 bucket, for easy data download by users.\n",
    "\n",
    "In the test below, after loading the dataset, the benchmark loops over all structures, performing single point calculations. The results are then stored in the `atoms.info`, and the structures are saved as .xyz files. Saving the results in this way allows for easy structure visualisation in the app later.\n",
    "\n",
    "Importantly, **no analysis is done here**, only the calculation and saving of energies (other examples may save forces, stresses, phonon frequencies etc.). All analysis is done in the analysis step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63606942",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"mlip\", MODELS.items())\n",
    "def test_lattice_energy(mlip: tuple[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Run X23 lattice energy test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mlip\n",
    "        Name of model use and model to get calculator.\n",
    "    \"\"\"\n",
    "    model_name, model = mlip\n",
    "    calc = model.get_calculator()\n",
    "\n",
    "    # Add D3 calculator for this test (for models where applicable)\n",
    "    calc = model.add_d3_calculator(calc)\n",
    "\n",
    "    # load X23 dataset\n",
    "    lattice_energy_dir = \"~/.cache/ml-peg/X23/lattice_energy\"\n",
    "\n",
    "    with open(lattice_energy_dir / \"list\") as f:\n",
    "        systems = f.read().splitlines()\n",
    "\n",
    "    for system in systems:\n",
    "        molecule_path = lattice_energy_dir / system / \"POSCAR_molecule\"\n",
    "        solid_path = lattice_energy_dir / system / \"POSCAR_solid\"\n",
    "        ref_path = lattice_energy_dir / system / \"lattice_energy_DMC\"\n",
    "        num_molecules_path = lattice_energy_dir / system / \"nmol\"\n",
    "\n",
    "        molecule = read(molecule_path, index=0, format=\"vasp\")\n",
    "        molecule.calc = calc\n",
    "        molecule.get_potential_energy()\n",
    "\n",
    "        solid = read(solid_path, index=0, format=\"vasp\")\n",
    "        solid.calc = copy(calc)\n",
    "        solid.get_potential_energy()\n",
    "\n",
    "        ref = np.loadtxt(ref_path)[0]\n",
    "        num_molecules = np.loadtxt(num_molecules_path)\n",
    "\n",
    "        solid.info[\"ref\"] = ref\n",
    "        solid.info[\"num_molecules\"] = num_molecules\n",
    "        solid.info[\"system\"] = system\n",
    "        molecule.info[\"ref\"] = ref\n",
    "        molecule.info[\"num_molecules\"] = num_molecules\n",
    "        molecule.info[\"system\"] = system\n",
    "\n",
    "        # Write output structures\n",
    "        write_dir = OUT_PATH / model_name\n",
    "        write_dir.mkdir(parents=True, exist_ok=True)\n",
    "        write(write_dir / f\"{system}.xyz\", [solid, molecule])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b9b5b",
   "metadata": {},
   "source": [
    "You can now run your benchmark through the CLI:\n",
    "\n",
    "```bash\n",
    "    ml_peg calc --category molecular_crystal --test X23\n",
    "```\n",
    "See the [CLI documentation](https://ddmms.github.io/ml-peg/developer_guide/running.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b008bf",
   "metadata": {},
   "source": [
    "Once the benchmark is run, you should see your output files in `ml_peg/calcs/<category>/your_benchmark/outputs/`,structured in the following way:\n",
    "```bash\n",
    "    calcs/\n",
    "    └── molecular_crystal/\n",
    "        └── X23/\n",
    "            └── outputs/\n",
    "                └── mace-mp-0a/\n",
    "                    └── structure_1.xyz\n",
    "                    └── structure_2.xyz\n",
    "                    └── ...\n",
    "                └── mace-mp-0b3/\n",
    "                    └── structure_1.xyz\n",
    "                    └── structure_2.xyz\n",
    "                    └── ...\n",
    "                └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dcc55",
   "metadata": {},
   "source": [
    "### 1.1 Adding pre-computed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6682e",
   "metadata": {},
   "source": [
    "In some cases, you may wish to add pre-computed data to ML-PEG without defining a calc script. This is not encouraged for benchmarks which fall into the short, medium or long compute time categories, but may be acceptable for very long compute time benchmarks where data generation is very expensive.\n",
    "\n",
    "In this case, you can create the output directory structure as above, and place your pre-computed files in the relevant model sub-directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a011a20",
   "metadata": {},
   "source": [
    "### 2. Analysis step: computing metrics and building plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17989d",
   "metadata": {},
   "source": [
    "In the analysis step, we take the calc outputs and prepare tables and plots for the app. First create a new directory for your benchmark in `ml_peg/analysis/<category>/your_benchmark/` and create a new python script `analyse_your_benchmark.py` in this directory.\n",
    "\n",
    "You also need to define `metrics.yml` in the same directory, specifying the metrics you compute, good and bad normalisation thresholds, units, a tooltip (column title hover text) and the level of theory of the reference data for this metric. If the metric is unitless, use `unit: null`. The example for X23 in `analysis/molecular_crystal/X23/metrics.yml`:\n",
    "```yaml\n",
    "    metrics:\n",
    "        MAE:\n",
    "            good: 0.0\n",
    "            bad: 100.0\n",
    "            unit: kJ/mol\n",
    "            tooltip: \"Mean Absolute Error for all systems\"\n",
    "            level_of_theory: DMC\n",
    "```\n",
    "For more information on the good and bad thresholds (what are they and how to choose defaults), see the [normalisation documentation](https://ddmms.github.io/ml-peg/developer_guide/normalisation.html). ADD CORRECT URL\n",
    "\n",
    "Again, the imports are fairly standard across benchmarks, along with loading models and defining data/output paths. The key difference is for benchmarks which use dispersion corrections, its required to define `D3_MODEL_NAMES = build_d3_name_map(MODELS)` so that model names appear with `+D3` suffix in the app table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6304346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyse X23 benchmark.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from ase import units\n",
    "from ase.io import read, write\n",
    "import pytest\n",
    "\n",
    "from ml_peg.analysis.utils.decorators import build_table, plot_parity\n",
    "from ml_peg.analysis.utils.utils import build_d3_name_map, load_metrics_config, mae\n",
    "from ml_peg.app import APP_ROOT\n",
    "from ml_peg.calcs import CALCS_ROOT\n",
    "from ml_peg.models.get_models import get_model_names\n",
    "from ml_peg.models.models import current_models\n",
    "\n",
    "MODELS = get_model_names(current_models)\n",
    "D3_MODEL_NAMES = build_d3_name_map(MODELS)\n",
    "CALC_PATH = CALCS_ROOT / \"molecular_crystal\" / \"X23\" / \"outputs\"\n",
    "OUT_PATH = APP_ROOT / \"data\" / \"molecular_crystal\" / \"X23\"\n",
    "\n",
    "METRICS_CONFIG_PATH = Path(__file__).with_name(\"metrics.yml\")\n",
    "DEFAULT_THRESHOLDS, DEFAULT_TOOLTIPS, DEFAULT_WEIGHTS = load_metrics_config(\n",
    "    METRICS_CONFIG_PATH\n",
    ")\n",
    "\n",
    "# Unit conversion\n",
    "EV_TO_KJ_PER_MOL = units.mol / units.kJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db027fd6",
   "metadata": {},
   "source": [
    "We define a function to retrieve system names, which is used to label points in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_names() -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of X23 system names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        List of system names from structure files.\n",
    "    \"\"\"\n",
    "    system_names = []\n",
    "    for model_name in MODELS:\n",
    "        model_dir = CALC_PATH / model_name\n",
    "        if model_dir.exists():\n",
    "            xyz_files = sorted(model_dir.glob(\"*.xyz\"))\n",
    "            if xyz_files:\n",
    "                for xyz_file in xyz_files:\n",
    "                    atoms = read(xyz_file)\n",
    "                    system_names.append(atoms.info[\"system\"])\n",
    "                break\n",
    "    return system_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4844b5",
   "metadata": {},
   "source": [
    "We use pytest fixtures to build a dependency chain, building up from the raw data. The first level above raw data is plotting the predicted vs reference energies scatter plot.\n",
    "\n",
    "We first define a function `lattice_energies()` to retrieve predicted and reference lattice energies from the calc outputs. The `@plot_parity` decorator tells the analysis framework to build a parity plot from the returned predicted and reference energies in a reproducible way. Here we add infomation such as the plot output path, title, axis labels and hover data (data shown when hovering over a point in the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76231a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "@plot_parity(\n",
    "    filename=OUT_PATH / \"figure_lattice_energies.json\",\n",
    "    title=\"X23 Lattice Energies\",\n",
    "    x_label=\"Predicted lattice energy / kJ/mol\",\n",
    "    y_label=\"Reference lattice energy / kJ/mol\",\n",
    "    hoverdata={\n",
    "        \"System\": get_system_names(),\n",
    "    },\n",
    ")\n",
    "def lattice_energies() -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Get lattice energies for all X23 systems.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, list]\n",
    "        Dictionary of reference and predicted lattice energies.\n",
    "    \"\"\"\n",
    "    results = {\"ref\": []} | {mlip: [] for mlip in MODELS}\n",
    "    ref_stored = False\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        model_dir = CALC_PATH / model_name\n",
    "\n",
    "        if not model_dir.exists():\n",
    "            continue\n",
    "\n",
    "        xyz_files = sorted(model_dir.glob(\"*.xyz\"))\n",
    "        if not xyz_files:\n",
    "            continue\n",
    "\n",
    "        for xyz_file in xyz_files:\n",
    "            structs = read(xyz_file, index=\":\")\n",
    "\n",
    "            solid_energy = structs[0].get_potential_energy()\n",
    "            num_molecules = structs[0].info[\"num_molecules\"]\n",
    "            system = structs[0].info[\"system\"]\n",
    "            molecule_energy = structs[1].get_potential_energy()\n",
    "\n",
    "            lattice_energy = (solid_energy / num_molecules) - molecule_energy\n",
    "            results[model_name].append(lattice_energy * EV_TO_KJ_PER_MOL)\n",
    "\n",
    "            # Copy individual structure files to app data directory\n",
    "            structs_dir = OUT_PATH / model_name\n",
    "            structs_dir.mkdir(parents=True, exist_ok=True)\n",
    "            write(structs_dir / f\"{system}.xyz\", structs)\n",
    "\n",
    "            # Store reference energies (only once)\n",
    "            if not ref_stored:\n",
    "                results[\"ref\"].append(structs[0].info[\"ref\"])\n",
    "\n",
    "        ref_stored = True\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e890c",
   "metadata": {},
   "source": [
    "Moving up the levels of abstraction, we define a function to compute the MAE metric from the predicted and reference lattice energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def x23_errors(lattice_energies) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get mean absolute error for lattice energies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lattice_energies\n",
    "        Dictionary of reference and predicted lattice energies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, float]\n",
    "        Dictionary of predicted lattice energy errors for all models.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for model_name in MODELS:\n",
    "        if lattice_energies[model_name]:\n",
    "            results[model_name] = mae(\n",
    "                lattice_energies[\"ref\"], lattice_energies[model_name]\n",
    "            )\n",
    "        else:\n",
    "            results[model_name] = None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557df163",
   "metadata": {},
   "source": [
    "Next, we build a results table from the MAEs using the `@build_table` decorator. For tests not using dispersion corrections, `mlip_name_map` can be omitted or set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "@build_table(\n",
    "    filename=OUT_PATH / \"x23_metrics_table.json\",\n",
    "    metric_tooltips=DEFAULT_TOOLTIPS,\n",
    "    thresholds=DEFAULT_THRESHOLDS,\n",
    "    mlip_name_map=D3_MODEL_NAMES,\n",
    ")\n",
    "def metrics(x23_errors: dict[str, float]) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Get all X23 metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x23_errors\n",
    "        Mean absolute errors for all systems.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict]\n",
    "        Metric names and values for all models.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"MAE\": x23_errors,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252ef77",
   "metadata": {},
   "source": [
    "This final fucntion must begin with `test_` to be discovered by `pytest` and execute the analysis chain. You can see how the chain is formed from the function arguments, where each argument is a dependency on a previous fixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x23(metrics: dict[str, dict]) -> None:\n",
    "    \"\"\"\n",
    "    Run X23 test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics\n",
    "        All X23 metrics.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5af7e3",
   "metadata": {},
   "source": [
    "You can now run the analysis through the CLI:\n",
    "\n",
    "```bash\n",
    "    ml_peg analyse --category molecular_crystal --test X23\n",
    "```\n",
    "See the [CLI documentation](https://ddmms.github.io/ml-peg/developer_guide/running.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f026",
   "metadata": {},
   "source": [
    "### 3. App step: building the interactive app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b51f08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlip-testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
