{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8a0986",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ddmms/ml-peg/blob/main/docs/source/tutorials/python/adding_benchmark.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c43a8f",
   "metadata": {},
   "source": [
    "# Adding a benchmark to ML-PEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f147716a",
   "metadata": {},
   "source": [
    "This notebook guides you through the process of adding a new benchmark to ML-PEG with interactive capabilities. The benchmark added will have the simplest form of interactivity, which is table -> scatter -> structure, meaning clicking a table cell shows a scatter plot from which the errors were calculated, and clicking a point on the scatter plot shows the corresponding structure. Different types (e.g. density plots) or more advanced interactivity (table -> scatter -> phonon dispersion) can also be achieved but will not be covered in this first introduction.\n",
    "\n",
    "The workflow consists of 3 main steps + documentation:\n",
    "- **calc**: where the benchmarking script is defined, typically outputting results as .xyz files (e.g. energies for a structure are stored as properties in the .xyz file)\n",
    "- **analysis**: takes the outputs from calc, computes relevant metrics, builds tables and plots (saved as .json files).\n",
    "- **app**: builds the interactive app using the analysis output plots/tables and structure files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a492f39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f8de0",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d2f64",
   "metadata": {},
   "source": [
    "### 0.0 Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba154b9",
   "metadata": {},
   "source": [
    "If you are following this tutorial using **Google Colab**, uncomment the following cells to clone the repository and install ML-PEG and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/ddmms/ml-peg\n",
    "# %cd ml-peg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import locale\n",
    "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# ! pip uninstall numpy -y # Uninstall pre-installed numpy\n",
    "\n",
    "# ! pip uninstall torch torchaudio torchvision transformers -y # Uninstall pre-installed torch\n",
    "# ! uv pip install torch==2.5.1 # Install pinned version of torch\n",
    "\n",
    "# ! uv pip install ml-peg[mace,orb,d3] --system # Install ML-PEG with MACE, Orb, and torch-dftd\n",
    "\n",
    "# get_ipython().kernel.do_shutdown(restart=True) # Restart kernel to update libraries. This may warn that your session has crashed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9f08e",
   "metadata": {},
   "source": [
    "If you have cloned the repository and are following this tutorial **locally**, we strongly recommend using `uv sync` as described in our [online documentation](https://ddmms.github.io/ml-peg/developer_guide/get_started.html).\n",
    "\n",
    "For troubleshooting, please also refer to the `uv` documentation interactions with [Jupyter/VS Code](https://docs.astral.sh/uv/guides/integration/jupyter/).\n",
    "\n",
    "Installing our pre-commit hooks is recommended, as this helps ensure contributions meet our guidelines. This requires additional dependencies that are installed by `uv sync`, but are not included when running `uv pip install ml_peg` in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pre-commit install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce721fe",
   "metadata": {},
   "source": [
    "### 0.1 GitHub\n",
    "\n",
    "To contribute a new benchmark, first open an issue on GitHub by going to Issues -> New issue -> New benchmark, where you can briefly describe the benchmark you wish to add. After you have created the issue, click on the issue and assign yourself to it (top RHS of page under \"Assignees\").\n",
    "\n",
    "You can then create a new branch for your benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f033938",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git checkout main\n",
    "! git pull origin main\n",
    "! git checkout -b my_new_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c305727",
   "metadata": {},
   "source": [
    "### 0.2 Setting up your directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d841459",
   "metadata": {},
   "source": [
    "In the ML-PEG, we organise benchmarks into *categories*, which make up the tabs in the app, along with the summary page. For example, the Molecular Crystals category consists of molecular crystal benchmarks e.g. DMC-ICE13 and X23.\n",
    "\n",
    "First see if your benchmark fits into an existing category. If so, create a new benchmark folder in the relevant category folders in `ml_peg/calcs/<category>/`, `ml_peg/analysis/<category>/` and `ml_peg/app/<category>/`, and move onto the next steps.\n",
    "\n",
    "If not, create a new category folder in `ml_peg/calcs/`, `ml_peg/analysis/` and `ml_peg/app/`. You'll also need to create a simple app config file in `ml_peg/app/<category>/` e.g. `ml_peg/app/molecular_crystal/molecular_crystal.yml`:\n",
    "```yaml\n",
    "    title: Molecular Crystals\n",
    "    description: Formation energies of molecular crystals\n",
    "```\n",
    "and add the category directory name to `docs/source/user_guide/benchmarks/index.rst` so that it appears in the documentation sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2666663",
   "metadata": {},
   "source": [
    "### 0.3 Documentation\n",
    "Before adding a new benchmark, it is good practice to add a documentation page for it. You can follow the format of existing benchmark documentation pages in `docs/source/user_guide/benchmarks/`. This is the example for the X23 molecular crystal benchmark we will look at in this notebook `docs/source/user_guide/benchmarks/molecular_crystals.rst`: \n",
    "\n",
    "```rst\n",
    "    ==================\n",
    "    Molecular Crystals\n",
    "    ==================\n",
    "\n",
    "    X23\n",
    "    ===\n",
    "\n",
    "    Summary\n",
    "    -------\n",
    "\n",
    "    Performance in predicting lattice energies for 23 molecular crystals.\n",
    "\n",
    "\n",
    "    Metrics\n",
    "    -------\n",
    "\n",
    "    1. Lattice energy error\n",
    "\n",
    "    Accuracy of lattice energy predictions.\n",
    "\n",
    "    For each molecular crystal, lattice energy is calculated by taking the difference\n",
    "    between the energy of the solid molecular crystal divided by the number of molecules it\n",
    "    comprises, and the energy of the isolated molecule. This is compared to the reference\n",
    "    lattice energy.\n",
    "\n",
    "\n",
    "    Computational cost\n",
    "    ------------------\n",
    "\n",
    "    Low: tests are likely to take less than a minute to run on CPU.\n",
    "\n",
    "\n",
    "    Data availability\n",
    "    -----------------\n",
    "\n",
    "    Input structures:\n",
    "\n",
    "    * A. M. Reilly and A. Tkatchenko, Understanding the role of vibrations, exact exchange,\n",
    "    and many-body van der waals interactions in the cohesive properties of molecular\n",
    "    crystals, The Journal of chemical physics 139 (2013).\n",
    "\n",
    "    Reference data:\n",
    "\n",
    "    * Same as input data\n",
    "    * DMC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3e32c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e03ed5",
   "metadata": {},
   "source": [
    "## 1. Calc step: defining the benchmarking script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac7bd7",
   "metadata": {},
   "source": [
    "First create a directory for your benchmark in `ml_peg/calcs/<category>/your_benchmark/` and create a new Python script `calc_your_benchmark.py` in this directory. This is where you will define your benchmarking script. Please use underscores, not hyphens, in the benchmark name to avoid import errors when building the final app.\n",
    "\n",
    "Now prepare your imports, load models (from `models/models.yml`), define data and output paths, and any unit conversions you may need. This setup is fairly standard across all benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run calculations for X23 tests.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from ase import units\n",
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "from ml_peg.calcs.utils.utils import download_s3_data\n",
    "from ml_peg.models.get_models import load_models\n",
    "from ml_peg.models.models import current_models\n",
    "\n",
    "MODELS = load_models(current_models)\n",
    "\n",
    "DATA_PATH = Path(__file__).parent / \"data\"\n",
    "OUT_PATH = Path(__file__).parent / \"outputs\"\n",
    "\n",
    "# Unit conversion\n",
    "EV_TO_KJ_PER_MOL = units.mol / units.kJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb19d5",
   "metadata": {},
   "source": [
    "Now you can define your benchmarking function. We use `pytest` to run all benchmarks in ML-PEG, so the function **must** begin with `test_` to be automatically discovered by `pytest`. A `pytest` decorator is also used to loop this test over all specified models.\n",
    "\n",
    "For this example, `add_d3_calculator` is used to add dispersion corrections to models where applicable. This is being expanded to include other dispersion corrections.\n",
    "\n",
    "Input data: We typically store data required to run calculations in an S3 bucket, which can be accessed using the `download_s3_data` function. As this uses the cache to prevent unnecessary downloads, for development you can place a zipped version of your input data in your cache folder e.g. `\"~/.cache/ml-peg/lattice_energy.zip\"` to test this function before the data has been uploaded.\n",
    "\n",
    "When the benchmark is finalised and ready to be merged with the main branch, please ask for your data to be uploaded to the S3 bucket, for easy data download by users. This will be saved in the form `inputs/<category>/your_benchmark/your_benchmark.zip`, even if `your_benchmark.zip` differs to the original filename as in this case.\n",
    "\n",
    "In the test below, after loading the dataset, the benchmark loops over all structures, performing single point calculations. The results are then stored in the `atoms.info`, and the structures are saved as .xyz files. Saving the results in this way allows for easy structure visualisation in the app later.\n",
    "\n",
    "Importantly, **no analysis is done here**, only the calculation and saving of energies (other examples may save forces, stresses, phonon frequencies etc.). All analysis is done in the analysis step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63606942",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"mlip\", MODELS.items())\n",
    "def test_lattice_energy(mlip: tuple[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Run X23 lattice energy test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mlip\n",
    "        Name of model use and model to get calculator.\n",
    "    \"\"\"\n",
    "    model_name, model = mlip\n",
    "    calc = model.get_calculator()\n",
    "\n",
    "    # Add D3 calculator for this test (for models where applicable)\n",
    "    calc = model.add_d3_calculator(calc)\n",
    "\n",
    "    # Download X23 dataset\n",
    "    lattice_energy_dir = (\n",
    "        download_s3_data(\n",
    "            key=\"inputs/molecular_crystal/X23/X23.zip\",\n",
    "            filename=\"lattice_energy.zip\",\n",
    "        )\n",
    "        / \"lattice_energy\"\n",
    "    )\n",
    "\n",
    "    with open(lattice_energy_dir / \"list\") as f:\n",
    "        systems = f.read().splitlines()\n",
    "\n",
    "    for system in systems:\n",
    "        molecule_path = lattice_energy_dir / system / \"POSCAR_molecule\"\n",
    "        solid_path = lattice_energy_dir / system / \"POSCAR_solid\"\n",
    "        ref_path = lattice_energy_dir / system / \"lattice_energy_DMC\"\n",
    "        num_molecules_path = lattice_energy_dir / system / \"nmol\"\n",
    "\n",
    "        molecule = read(molecule_path, index=0, format=\"vasp\")\n",
    "        molecule.calc = calc\n",
    "        molecule.get_potential_energy()\n",
    "\n",
    "        solid = read(solid_path, index=0, format=\"vasp\")\n",
    "        solid.calc = copy(calc)\n",
    "        solid.get_potential_energy()\n",
    "\n",
    "        ref = np.loadtxt(ref_path)[0]\n",
    "        num_molecules = np.loadtxt(num_molecules_path)\n",
    "\n",
    "        solid.info[\"ref\"] = ref\n",
    "        solid.info[\"num_molecules\"] = num_molecules\n",
    "        solid.info[\"system\"] = system\n",
    "        molecule.info[\"ref\"] = ref\n",
    "        molecule.info[\"num_molecules\"] = num_molecules\n",
    "        molecule.info[\"system\"] = system\n",
    "\n",
    "        # Write output structures\n",
    "        write_dir = OUT_PATH / model_name\n",
    "        write_dir.mkdir(parents=True, exist_ok=True)\n",
    "        write(write_dir / f\"{system}.xyz\", [solid, molecule])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b9b5b",
   "metadata": {},
   "source": [
    "You can now run your benchmark through the CLI:\n",
    "\n",
    "```bash\n",
    "    ml_peg calc --category molecular_crystal --test X23\n",
    "```\n",
    "See the [CLI documentation](https://ddmms.github.io/ml-peg/developer_guide/running.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b008bf",
   "metadata": {},
   "source": [
    "Once the benchmark is run, you should see your output files in `ml_peg/calcs/<category>/your_benchmark/outputs/`,structured in the following way:\n",
    "```bash\n",
    "    calcs/\n",
    "    └── molecular_crystal/\n",
    "        └── X23/\n",
    "            └── outputs/\n",
    "                └── mace-mp-0a/\n",
    "                    └── structure_1.xyz\n",
    "                    └── structure_2.xyz\n",
    "                    └── ...\n",
    "                └── mace-mp-0b3/\n",
    "                    └── structure_1.xyz\n",
    "                    └── structure_2.xyz\n",
    "                    └── ...\n",
    "                └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dcc55",
   "metadata": {},
   "source": [
    "### 1.1 Adding pre-computed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6682e",
   "metadata": {},
   "source": [
    "In some cases, you may wish to add pre-computed data to ML-PEG without defining a calc script. This is not encouraged for benchmarks which fall into the short, medium or long compute time categories, but may be acceptable for very long compute time benchmarks where data generation is very expensive.\n",
    "\n",
    "In this case, you can create the output directory structure as above, and place your pre-computed files in the relevant model sub-directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a011a20",
   "metadata": {},
   "source": [
    "## 2. Analysis step: computing metrics and building plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17989d",
   "metadata": {},
   "source": [
    "In the analysis step, we take the calc outputs and prepare tables and plots for the app. First create a new directory for your benchmark in `ml_peg/analysis/<category>/your_benchmark/` and create a new python script `analyse_your_benchmark.py` in this directory.\n",
    "\n",
    "You also need to define `metrics.yml` in the same directory, specifying the metrics you compute, good and bad normalisation thresholds, units, a tooltip (column title hover text) and the level of theory of the reference data for this metric. If the metric is unitless, use `unit: null`. The example for X23 in `analysis/molecular_crystal/X23/metrics.yml`:\n",
    "```yaml\n",
    "    metrics:\n",
    "        MAE:\n",
    "            good: 0.0\n",
    "            bad: 100.0\n",
    "            unit: kJ/mol\n",
    "            tooltip: \"Mean Absolute Error for all systems\"\n",
    "            level_of_theory: DMC\n",
    "```\n",
    "For more information on the good and bad thresholds (what are they and how to choose defaults), see the [normalisation documentation](https://ddmms.github.io/ml-peg/developer_guide/scoring_and_normalisation.html).\n",
    "\n",
    "Again, the imports are fairly standard across benchmarks, along with loading models and defining data/output paths. The key difference is for benchmarks which use dispersion corrections, its required to define `D3_MODEL_NAMES = build_d3_name_map(MODELS)` so that model names appear with `+D3` suffix in the app table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6304346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyse X23 benchmark.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from ase import units\n",
    "from ase.io import read, write\n",
    "import pytest\n",
    "\n",
    "from ml_peg.analysis.utils.decorators import build_table, plot_parity\n",
    "from ml_peg.analysis.utils.utils import build_d3_name_map, load_metrics_config, mae\n",
    "from ml_peg.app import APP_ROOT\n",
    "from ml_peg.calcs import CALCS_ROOT\n",
    "from ml_peg.models.get_models import get_model_names\n",
    "from ml_peg.models.models import current_models\n",
    "\n",
    "MODELS = get_model_names(current_models)\n",
    "D3_MODEL_NAMES = build_d3_name_map(MODELS)\n",
    "CALC_PATH = CALCS_ROOT / \"molecular_crystal\" / \"X23\" / \"outputs\"\n",
    "OUT_PATH = APP_ROOT / \"data\" / \"molecular_crystal\" / \"X23\"\n",
    "\n",
    "METRICS_CONFIG_PATH = Path(__file__).with_name(\"metrics.yml\")\n",
    "DEFAULT_THRESHOLDS, DEFAULT_TOOLTIPS, DEFAULT_WEIGHTS = load_metrics_config(\n",
    "    METRICS_CONFIG_PATH\n",
    ")\n",
    "\n",
    "# Unit conversion\n",
    "EV_TO_KJ_PER_MOL = units.mol / units.kJ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db027fd6",
   "metadata": {},
   "source": [
    "We define a function to retrieve system names, which is used to label points in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_names() -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of X23 system names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        List of system names from structure files.\n",
    "    \"\"\"\n",
    "    system_names = []\n",
    "    for model_name in MODELS:\n",
    "        model_dir = CALC_PATH / model_name\n",
    "        if model_dir.exists():\n",
    "            xyz_files = sorted(model_dir.glob(\"*.xyz\"))\n",
    "            if xyz_files:\n",
    "                for xyz_file in xyz_files:\n",
    "                    atoms = read(xyz_file)\n",
    "                    system_names.append(atoms.info[\"system\"])\n",
    "                break\n",
    "    return system_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4844b5",
   "metadata": {},
   "source": [
    "We use pytest fixtures to build a dependency chain, building up from the raw data. The first level above raw data is plotting the predicted vs reference energies scatter plot.\n",
    "\n",
    "We first define a function `lattice_energies()` to retrieve predicted and reference lattice energies from the calc outputs. The `@plot_parity` decorator tells the analysis framework to build a parity plot from the returned predicted and reference energies in a reproducible way. Here we add infomation such as the plot output path, title, axis labels and hover data (data shown when hovering over a point in the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76231a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "@plot_parity(\n",
    "    filename=OUT_PATH / \"figure_lattice_energies.json\",\n",
    "    title=\"X23 Lattice Energies\",\n",
    "    x_label=\"Predicted lattice energy / kJ/mol\",\n",
    "    y_label=\"Reference lattice energy / kJ/mol\",\n",
    "    hoverdata={\n",
    "        \"System\": get_system_names(),\n",
    "    },\n",
    ")\n",
    "def lattice_energies() -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Get lattice energies for all X23 systems.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, list]\n",
    "        Dictionary of reference and predicted lattice energies.\n",
    "    \"\"\"\n",
    "    results = {\"ref\": []} | {mlip: [] for mlip in MODELS}\n",
    "    ref_stored = False\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        model_dir = CALC_PATH / model_name\n",
    "\n",
    "        if not model_dir.exists():\n",
    "            continue\n",
    "\n",
    "        xyz_files = sorted(model_dir.glob(\"*.xyz\"))\n",
    "        if not xyz_files:\n",
    "            continue\n",
    "\n",
    "        for xyz_file in xyz_files:\n",
    "            structs = read(xyz_file, index=\":\")\n",
    "\n",
    "            solid_energy = structs[0].get_potential_energy()\n",
    "            num_molecules = structs[0].info[\"num_molecules\"]\n",
    "            system = structs[0].info[\"system\"]\n",
    "            molecule_energy = structs[1].get_potential_energy()\n",
    "\n",
    "            lattice_energy = (solid_energy / num_molecules) - molecule_energy\n",
    "            results[model_name].append(lattice_energy * EV_TO_KJ_PER_MOL)\n",
    "\n",
    "            # Copy individual structure files to app data directory\n",
    "            structs_dir = OUT_PATH / model_name\n",
    "            structs_dir.mkdir(parents=True, exist_ok=True)\n",
    "            write(structs_dir / f\"{system}.xyz\", structs)\n",
    "\n",
    "            # Store reference energies (only once)\n",
    "            if not ref_stored:\n",
    "                results[\"ref\"].append(structs[0].info[\"ref\"])\n",
    "\n",
    "        ref_stored = True\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e890c",
   "metadata": {},
   "source": [
    "Moving up the levels of abstraction, we define a function to compute the MAE metric from the predicted and reference lattice energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def x23_errors(lattice_energies) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get mean absolute error for lattice energies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lattice_energies\n",
    "        Dictionary of reference and predicted lattice energies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, float]\n",
    "        Dictionary of predicted lattice energy errors for all models.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for model_name in MODELS:\n",
    "        if lattice_energies[model_name]:\n",
    "            results[model_name] = mae(\n",
    "                lattice_energies[\"ref\"], lattice_energies[model_name]\n",
    "            )\n",
    "        else:\n",
    "            results[model_name] = None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557df163",
   "metadata": {},
   "source": [
    "Next, we build a results table from the MAEs using the `@build_table` decorator. For tests not using dispersion corrections, `mlip_name_map` can be omitted or set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "@build_table(\n",
    "    filename=OUT_PATH / \"x23_metrics_table.json\",\n",
    "    metric_tooltips=DEFAULT_TOOLTIPS,\n",
    "    thresholds=DEFAULT_THRESHOLDS,\n",
    "    mlip_name_map=D3_MODEL_NAMES,\n",
    ")\n",
    "def metrics(x23_errors: dict[str, float]) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Get all X23 metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x23_errors\n",
    "        Mean absolute errors for all systems.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, dict]\n",
    "        Metric names and values for all models.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"MAE\": x23_errors,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252ef77",
   "metadata": {},
   "source": [
    "This final fucntion must begin with `test_` to be discovered by `pytest` and execute the analysis chain. You can see how the chain is formed from the function arguments, where each argument is a dependency on a previous fixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_x23(metrics: dict[str, dict]) -> None:\n",
    "    \"\"\"\n",
    "    Run X23 test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics\n",
    "        All X23 metrics.\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5af7e3",
   "metadata": {},
   "source": [
    "You can now run the analysis through the CLI:\n",
    "\n",
    "```bash\n",
    "    ml_peg analyse --category molecular_crystal --test X23\n",
    "```\n",
    "See the [CLI documentation](https://ddmms.github.io/ml-peg/developer_guide/running.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0f026",
   "metadata": {},
   "source": [
    "## 3. App step: building the interactive app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b51f08",
   "metadata": {},
   "source": [
    "Now that we have prepared and saved the table and plots for our new benchmark, the final step is to create an application, which defines the layout and interactivity for users to explore.\n",
    "\n",
    "First create a new directory for your benchmark in `ml_peg/app/<category>/your_benchmark/` and create a new Python script `app_your_benchmark.py` in this directory.\n",
    "\n",
    "As with calculations and analysis, the initial setup is relatively standard across benchmarks. The main things to change are:\n",
    "\n",
    "- `ml_peg.app.utils.build_callbacks` imports\n",
    "  - These will depend on the interactivity your benchmark requires\n",
    "- `BENCHMARK_NAME`\n",
    "  - This defines the human-friendly name for your benchmark\n",
    "- `DOCS_URL`\n",
    "  - This defines the link to benchmarks documentation\n",
    "  - The final part of the URL must be `[catgeory].html#[benchmark-subheading]`, where the subheading corresponds to the benchmark's subheading in the category's documentation .rst file. This may differ to name used for your benchmark directory, and any spaces are replaced with `-`. For example, [Elemental Slab Oxygen Adsorption](https://ddmms.github.io/ml-peg/user_guide/benchmarks/surfaces.html#elemental-slab-oxygen-adsorption) would have `surfaces.html#elemental-slab-oxygen-adsorption`.\n",
    "- `DATA_PATH`\n",
    "  - This should correspond to where data for the application has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run X23 app.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dash import Dash\n",
    "from dash.html import Div\n",
    "\n",
    "from ml_peg.app import APP_ROOT\n",
    "from ml_peg.app.base_app import BaseApp\n",
    "from ml_peg.app.utils.build_callbacks import (\n",
    "    plot_from_table_column,\n",
    "    struct_from_scatter,\n",
    ")\n",
    "from ml_peg.app.utils.load import read_plot\n",
    "from ml_peg.models.get_models import get_model_names\n",
    "from ml_peg.models.models import current_models\n",
    "\n",
    "# Get all models\n",
    "MODELS = get_model_names(current_models)\n",
    "BENCHMARK_NAME = \"X23 Lattice Energies\"\n",
    "DOCS_URL = (\n",
    "    \"https://ddmms.github.io/ml-peg/user_guide/benchmarks/molecular_crystal.html#x23\"\n",
    ")\n",
    "DATA_PATH = APP_ROOT / \"data\" / \"molecular_crystal\" / \"X23\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65caa0bd",
   "metadata": {},
   "source": [
    "Next, we define the benchmark app's layout and interactivity by creating a new class that inherits from `BaseApp`. `BaseApp` automatically handles building the app layout, so in practice the only requirement should be to define the `register_callbacks` function, which is how we specify the app's interactivity.\n",
    "\n",
    "In this case, we need to define the interactivity that enables:\n",
    "\n",
    "1. Clicking on the table showing the scatter plot\n",
    "\n",
    "For this, we use the `plot_from_table_column` function imported from `ml_peg.app.utils.build_callbacks`. This requires:\n",
    "\n",
    "- An ID for the data table being clicked, which is automatically defined for the app as `self.table_id`\n",
    "- A placeholder ID for the scatter plot. This should always include `BENCHMARK_NAME`, as IDs must be unique across all benchmark applications\n",
    "- A mapping from the table column name(s) to the scatter plot(s) that should be viewed when the corresponding column is clicked. Each dictionary key in `column_to_plot` corresponds to a table column header, and the dictionary value corresponds to the scatter plot loaded using `read_plot`. Note that we call `read_plot` within `register_callbacks`, as if there are any issues loading this, a non-interactive form of the data table can still be viewed when using the command-line interface\n",
    "\n",
    "2. Clicking on a point in the scatter plot showing the corresponding structure\n",
    "\n",
    "For this, we use the `struct_from_scatter` function `struct_from_scatter`imported from `ml_peg.app.utils.build_callbacks`. This requires:\n",
    "\n",
    "- The ID for the loaded scatter plot. This is the `id` passed to `read_plot`, not the placeholder ID, but similarly should include `BENCHMARK_NAME` to ensure it is unique\n",
    "- A unique placeholder ID for the visualisation\n",
    "- A list of structure files, in the same order as the scatter points where defined. These will be saved in `assets/[category]/[your benchmark]/[model name]`. Since the list of structures is the same for all models, in this case we simply find all of the structures saved for the first model, and sort this consistently with the sorting done within analysis. Note that `sorted` by default sorts lexicographically, not numerically, but this is not a problem if this is consistent with the order of the scatter points\n",
    "- Whether each structure file contains a single structure (`struct`), or multiple structures to be visualised per scatter point (`traj`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7553d",
   "metadata": {},
   "source": [
    "Further pre-defined callbacks are available in `ml_peg.app.utils.build_callbacks`, including cell-specific (rather than per column) interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X23App(BaseApp):\n",
    "    \"\"\"X23 benchmark app layout and callbacks.\"\"\"\n",
    "\n",
    "    def register_callbacks(self) -> None:\n",
    "        \"\"\"Register callbacks to app.\"\"\"\n",
    "        scatter = read_plot(\n",
    "            DATA_PATH / \"figure_lattice_energies.json\",\n",
    "            id=f\"{BENCHMARK_NAME}-figure\",\n",
    "        )\n",
    "\n",
    "        # Assets dir will be parent directory - individual files for each system\n",
    "        structs_dir = DATA_PATH / MODELS[0]\n",
    "        structs = [\n",
    "            f\"assets/molecular_crystal/X23/{MODELS[0]}/{struct_file.stem}.xyz\"\n",
    "            for struct_file in sorted(structs_dir.glob(\"*.xyz\"))\n",
    "        ]\n",
    "\n",
    "        plot_from_table_column(\n",
    "            table_id=self.table_id,\n",
    "            plot_id=f\"{BENCHMARK_NAME}-figure-placeholder\",\n",
    "            column_to_plot={\"MAE\": scatter},\n",
    "        )\n",
    "\n",
    "        struct_from_scatter(\n",
    "            scatter_id=f\"{BENCHMARK_NAME}-figure\",\n",
    "            struct_id=f\"{BENCHMARK_NAME}-struct-placeholder\",\n",
    "            structs=structs,\n",
    "            mode=\"struct\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e765bb",
   "metadata": {},
   "source": [
    "We then create a function that instantiates our new app class, which **must** be called `get_app`. This is where we pass the global variables defined at the start of our script, as well as a description of the benchmark, and the name of the saved data table to be loaded.\n",
    "\n",
    "We also pass IDs of the placeholders for the additional interactive components we added above to `extra_components`, in the order that they should be displayed. In this case, these correspond to the placeholder for the scatter plot, followed by the placeholder for the structure visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app() -> X23App:\n",
    "    \"\"\"\n",
    "    Get X23 benchmark app layout and callback registration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X23App\n",
    "        Benchmark layout and callback registration.\n",
    "    \"\"\"\n",
    "    return X23App(\n",
    "        name=BENCHMARK_NAME,\n",
    "        description=\"Lattice energies for 23 organic molecular crystals.\",\n",
    "        docs_url=DOCS_URL,\n",
    "        table_path=DATA_PATH / \"x23_metrics_table.json\",\n",
    "        extra_components=[\n",
    "            Div(id=f\"{BENCHMARK_NAME}-figure-placeholder\"),\n",
    "            Div(id=f\"{BENCHMARK_NAME}-struct-placeholder\"),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882de00",
   "metadata": {},
   "source": [
    "Finally, we provide a way to build and run the application, involving getting the layout of the app, and registering the callbacks that we defined above.\n",
    "\n",
    "This section of code is **not** used when using the command-line interface to run the application, e.g.\n",
    "\n",
    "```bash\n",
    "    ml_peg app --category molecular_crystal\n",
    "```\n",
    "\n",
    "but is useful for testing your new bechmark's application.\n",
    "\n",
    "While the default port for Dash applications is 8050, we recommend setting a different port here, as you cannot run multiple applications on the same port, and this process is not always stopped by the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create Dash app\n",
    "    full_app = Dash(__name__, assets_folder=DATA_PATH.parent.parent)\n",
    "\n",
    "    # Construct layout and register callbacks\n",
    "    x23_app = get_app()\n",
    "    full_app.layout = x23_app.layout\n",
    "    x23_app.register_callbacks()\n",
    "\n",
    "    # Run app\n",
    "    full_app.run(port=8055, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de0e49",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bd464",
   "metadata": {},
   "source": [
    "We encourage you to read through our online [developer documentation](https://ddmms.github.io/ml-peg/developer_guide/index.html) for further help, including:\n",
    "\n",
    "- Setting up [developer tools](https://ddmms.github.io/ml-peg/developer_guide/get_started.html)\n",
    "- [Adding benchmarks](https://ddmms.github.io/ml-peg/developer_guide/add_benchmarks.html)\n",
    "  - This covers similar material to this tutorial, but provides more context on some underlying features such as the decorators used in analysis, and alternative calculation drivers\n",
    "- Running ML-PEGs [calculations, analysis, and application](https://ddmms.github.io/ml-peg/developer_guide/running.html)\n",
    "- [Adding data required for calculations](https://ddmms.github.io/ml-peg/developer_guide/data.html)\n",
    "- Benchmark [scoring and normalisation](https://ddmms.github.io/ml-peg/developer_guide/scoring_and_normalisation.html)\n",
    "\n",
    "You may also find it useful to refer to some [exemplar pull requests](https://github.com/ddmms/ml-peg/pulls?q=is%3Apr+label%3A%22example+benchmark+addition%22+), incuding examples of [data-only additions](https://github.com/ddmms/ml-peg/pulls?q=+is%3Apr+label%3A%22data+only%22+)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-peg (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
